{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f316a2fa",
   "metadata": {},
   "source": [
    " # Phi-3\n",
    " The first step is to load our model onto the GPU for faster inference. Note that we load the model and tokenizer separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ee398c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\2026-courses\\LLMs-Handson\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "d:\\2026-courses\\LLMs-Handson\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\pc\\.cache\\huggingface\\hub\\models--microsoft--Phi-3-mini-4k-instruct. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "W0124 18:00:56.770000 20328 Lib\\site-packages\\torch\\distributed\\elastic\\multiprocessing\\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Downloading shards:  50%|█████     | 1/2 [03:10<03:10, 190.21s/it]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Downloading shards: 100%|██████████| 2/2 [04:54<00:00, 147.41s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:45<00:00, 22.87s/it]\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=False\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c5a4f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Create a pipeline\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=False,\n",
    "    max_new_tokens=500,\n",
    "    do_sample=False\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f070aba8",
   "metadata": {},
   "source": [
    "## Check the tokenize's vocab size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec41d7e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size : 32000\n",
      "Vocab Size : 32011\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vocab Size : {tokenizer.vocab_size}\")\n",
    "print(f\"Vocab Size : {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1a2f73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Hello: [15043]\n",
      "Tokenized world: [3186]\n",
      "Tokenized AI: [319, 29902]\n",
      "Tokenized learning: [6509]\n",
      "Tokenized Artificial: [3012, 928, 616]\n",
      "Tokenized Intelligence: [3159, 28286]\n",
      "Tokenized is: [338]\n",
      "Tokenized transforming: [4327, 292]\n",
      "Tokenized the: [278]\n",
      "Tokenized world: [3186]\n",
      "Original Sentence : Artificial Intelligence is transforming the world\n",
      "Encoded Sentence : [3012, 928, 616, 3159, 28286, 338, 4327, 292, 278, 3186]\n",
      "Decoded Sentence : Artificial Intelligence is transforming the world\n"
     ]
    }
   ],
   "source": [
    "words = [\"Hello\", \"world\", \"AI\", \"learning\"]\n",
    "for word in words:\n",
    "    print(f\"Tokenized {word}: {tokenizer.encode(word)}\")\n",
    "\n",
    "sentence= \"Artificial Intelligence is transforming the world\"\n",
    "\n",
    "for word in sentence.split():\n",
    "    print(f\"Tokenized {word}: {tokenizer.encode(word)}\")\n",
    "\n",
    "encoded_sentence= tokenizer.encode(sentence)\n",
    "decoded_sentence= tokenizer.decode(encoded_sentence)\n",
    "print(f\"Original Sentence : {sentence}\")\n",
    "print(f\"Encoded Sentence : {encoded_sentence}\")\n",
    "print(f\"Decoded Sentence : {decoded_sentence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd10c0b4",
   "metadata": {},
   "source": [
    "## Encode the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02b6b5a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token ID : 3012 \t Token : Art\n",
      "Token ID : 928 \t Token : ific\n",
      "Token ID : 616 \t Token : ial\n",
      "Token ID : 3159 \t Token : Int\n",
      "Token ID : 28286 \t Token : elligence\n",
      "Token ID : 338 \t Token : is\n",
      "Token ID : 4327 \t Token : transform\n",
      "Token ID : 292 \t Token : ing\n",
      "Token ID : 278 \t Token : the\n",
      "Token ID : 3186 \t Token : world\n"
     ]
    }
   ],
   "source": [
    "encode_sentence = tokenizer.encode(sentence)\n",
    "# Decode each token id back to its respective token\n",
    "tokens = [tokenizer.decode([token_id]) for token_id in encode_sentence]\n",
    "# Create the token IDs and their corresponding tokens\n",
    "token_id_map = list(zip(encode_sentence, tokens))\n",
    "#Print the token IDs and their corresponding tokens\n",
    "for token_id, token in token_id_map:\n",
    "    print(f\"Token ID : {token_id} \\t Token : {token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c7a2038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Why did the chicken join the band? Because it had the drumsticks!\n"
     ]
    }
   ],
   "source": [
    "# The prompt (user input / query)\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Create a funny joke about chickens.\"}\n",
    "]\n",
    "\n",
    "# Generate output\n",
    "output = generator(messages)\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe45a79a",
   "metadata": {},
   "source": [
    "## Write an Email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ed32407a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write an email apologizing to Sarah for the tragic gerdening mishap. Explain how it happened.<|assistant|> Subject: Sincere Apologies for the Gerdening Mishap\n",
      "\n",
      "Dear Sarah,\n",
      "\n",
      "I hope this email finds you well. I am writing to express my deepest apologies for the unfortunate incident that occurred during our recent gerdening session. I understand that the mishap has caused you distress, and I want to assure you that I take full responsibility for what happened.\n",
      "\n",
      "The incident occurred due to a combination of factors, including a miscommunication between myself and the gerdening instructor, as well as a momentary lapse in concentration on my part. I failed to follow the proper instructions and ended up causing the accident.\n",
      "\n",
      "I am truly sorry for any pain or inconvenience this may have caused you. I understand that gerdening is a delicate process, and I should have been more attentive and cautious. I take full responsibility for my actions and the consequences that followed.\n",
      "\n",
      "To make amends, I would like to offer you a complimentary gerdening session with a highly qualified instructor who specializes in the area where the mishap occurred. I believe this will help you regain your confidence and continue your gerdening journey with peace of mind.\n",
      "\n",
      "In addition, I will personally oversee your next gerdening session to ensure that everything goes smoothly and that you receive the best possible care and guidance. I am committed to learning from this experience and improving my skills as a gerdening practitioner.\n",
      "\n",
      "Once again, I sincerely apologize for the unfortunate incident and the distress it has caused you. I appreciate your understanding and patience as I work to rectify the situation. Please let me know if there is anything else I can do to make things right.\n",
      "\n",
      "Thank you for your time, and I look forward to hearing from you soon.\n",
      "\n",
      "Sincerely,\n",
      "\n",
      "[Your Name]\n",
      "\n",
      "[Your Contact Information]<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Write an email apologizing to Sarah for the tragic gerdening mishap. Explain how it happened.<|assistant|>\"\n",
    "\n",
    "# Tokenize the input prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "# Generate text\n",
    "generation_output = model.generate(input_ids, max_new_tokens=500)\n",
    "\n",
    "print(tokenizer.decode(generation_output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bdaa1cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write an email apologizing to Azeddine explain to him the Semantic Web.<|assistant|> Subject: Understanding the Semantic Web and Its Significance\n",
      "\n",
      "Dear Azeddine,\n",
      "\n",
      "I hope this email finds you well. I wanted to take a moment to apologize for any confusion or misunderstanding that may have arisen from my previous communication. I understand that you may not be familiar with the concept of the Semantic Web, and I would like to provide you with a clear explanation to help you better understand its significance.\n",
      "\n",
      "The Semantic Web is an extension of the World Wide Web that enables machines to understand and interpret the meaning of information on the web. It is a collaborative effort to create a common framework that allows data to be shared and reused across various applications, enterprises, and communities.\n",
      "\n",
      "The main goal of the Semantic Web is to make the web more intelligent and useful by enabling machines to process and understand the meaning of information. This is achieved through the use of standardized formats and languages, such as the Resource Description Framework (RDF), Web Ontology Language (OWL), and SPARQL.\n",
      "\n",
      "RDF is a standard model for data interchange on the web, which allows data to be structured in a way that can be easily understood by machines. OWL is a language used to define ontologies, which are formal descriptions of concepts and relationships within a specific domain. SPARQL is a query language used to retrieve and manipulate data stored in RDF format.\n",
      "\n",
      "The Semantic Web has numerous applications, including improving search engine results, enabling more accurate data integration, and facilitating the development of intelligent applications. For example, it can help businesses to better understand their customers by analyzing data from various sources and providing more personalized recommendations.\n",
      "\n",
      "In conclusion, the Semantic Web is a powerful tool that has the potential to revolutionize the way we interact with information on the web. I hope this explanation has helped you to better understand its significance and potential applications. If you have any further questions or concerns, please do not hesitate to reach out to me.\n",
      "\n",
      "Thank you for your time and understanding.\n",
      "\n",
      "Best regards,\n",
      "\n",
      "[Your Name]<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Write an email apologizing to Azeddine explain to him the Semantic Web.<|assistant|>\"\n",
    "\n",
    "# Tokenize the input prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "# Generate text\n",
    "generation_output = model.generate(input_ids, max_new_tokens=500)\n",
    "\n",
    "print(tokenizer.decode(generation_output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712de7ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb134a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
